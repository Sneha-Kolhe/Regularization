{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738b820c-33d2-4b0d-9eea-aeb77383c716",
   "metadata": {},
   "source": [
    "## k What is regularization in the context of deep learningH Why is it importantG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00d207-183d-410a-935f-38fbf8b2398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preventing Overfitting:\n",
    "\n",
    "The primary goal of regularization is to prevent overfitting by reducing the complexity of the model and promoting smoother decision boundaries. Regularization techniques penalize large parameter values or complex model architectures, which discourages the network from learning overly complex representations that are specific to the training data.\n",
    "Improving Generalization:\n",
    "\n",
    "By preventing overfitting, regularization techniques help improve the generalization performance of neural networks, allowing them to make accurate predictions on unseen data. A well-regularized model is more likely to capture the underlying patterns and relationships in the data, rather than memorizing noise or outliers present in the training set.\n",
    "Handling Noisy Data:\n",
    "\n",
    "Regularization techniques help neural networks handle noisy or imperfect training data by learning more robust representations that are less sensitive to small variations or outliers. By discouraging the network from fitting noise in the data, regularization improves the model's ability to generalize to new, unseen examples.\n",
    "Reducing Variance:\n",
    "\n",
    "Regularization helps reduce the variance of the model by limiting the flexibility of the parameter space. This makes the model less prone to high variance, where small changes in the training data can lead to large fluctuations in the model's predictions.\n",
    "Controlling Model Complexity:\n",
    "\n",
    "Regularization techniques provide a way to control the complexity of the model and prevent it from becoming overly complex, which can lead to poor generalization. By adding regularization penalties to the loss function, the optimization process is guided towards simpler solutions that are more likely to generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d8fad-5afa-4ad0-96c1-6d0f59aaa932",
   "metadata": {},
   "source": [
    "## Ek Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoffk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b528773-e4fd-43f8-bd9e-51e4f6c93324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
